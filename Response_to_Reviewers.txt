RESPONSE TO REVIEWERS

Dear Reviewers,

We sincerely thank both reviewers for their constructive feedback. We address each concern systematically below.


1. RESPONSE TO COMMON CONCERN (Reviewers 1 & 2, Point 1): Evaluation Accuracy and Optimization Process

(1) Evaluation Methodology: In the CPJ framework, caption quality and answer selection are assessed through LLM-as-a-Judge scoring mechanisms with structured criteria. We validated reliability through manual verification of 10% randomly sampled instances, demonstrating high consistency between automated judgments and human annotations with no significant discrepancies.

(2) Public Code Release: To enhance reproducibility, we have released our complete implementation on GitHub, including source code for all pipeline stages with detailed prompts, evaluation criteria and scoring rubrics, sample datasets, and comprehensive documentation.

(3) Quantitative Results: For caption optimization effects, see Section 3.4 Ablation Study, Caption Optimization subsection, presenting improvements across accuracy, completeness, detail, and relevance.


2. RESPONSE TO REVIEWER 1, Point 2: Comparison with Traditional Detection Models

(1) Task Formulation: CPJ is designed for the CDDMBench VQA dataset (Section 3.1), addressing open-ended diagnostic questions requiring natural language responses.

(2) Metric Clarification: Table 1's metrics derive from keyword extraction within natural language responses, not direct classification from single-task models.

(3) Methodological Incompatibility: As discussed in Section 1 Introduction, YOLO models perform object detection and segmentation with categorical labels and cannot generate the free-form diagnostic responses required by VQA benchmarks. Direct comparison is therefore not applicable to our evaluation framework.


3. RESPONSE TO REVIEWER 2, Point 2 AND REVIEWER 1 (Reference Appropriateness): Citation Completeness

(1) Incorporation of Recommended Works: We greatly appreciate the suggested papers on Pruning All-Rounder (PAR) and In-Context Prompt Learning (InCPL), which align with our training-free paradigm. We will incorporate these citations in the revised manuscript and discuss their methodological connections to our approach.

(2) Relevance of Current References: Our bibliography is carefully curated for domain specificity, focusing exclusively on multimodal large language models, agricultural AI applications, and VQA evaluation methodologies. Each citation directly supports our technical contributions and experimental design, ensuring strong thematic coherence.

(3) Addressing Identified Omissions: We acknowledge the minor omissions noted by Reviewer 1, which do not affect novelty. We will supplement the bibliography in revision for comprehensive coverage of training-free optimization.


These clarifications, coupled with public code release, substantially enhance reproducibility and contextualize our contributions.

Thank you for your valuable feedback.

Sincerely,
The Authors
